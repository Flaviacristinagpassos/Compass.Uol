{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Este projeto implementa um contador de palavras utilizando o framework Apache Spark. O objetivo do exercício é demonstrar o uso de Spark para processar dados de maneira distribuída, utilizando um arquivo de texto como entrada, e contar o número de ocorrências de cada palavra.\n",
        "\n",
        "O código foi desenvolvido e executado no Google Colab, utilizando PySpark, e inclui o processamento de um arquivo de texto simples (README.md) para contar as palavras presentes no arquivo.\n"
      ],
      "metadata": {
        "id": "gcdqGbB4y_Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalação de Dependências**\n",
        "## Instalação do PySpark e do Java no ambiente do Google Colab para permitir o uso do Spark."
      ],
      "metadata": {
        "id": "Ov8RmJp4zNgr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiSrWs14K4QK",
        "outputId": "63cd00fe-b0c2-45e7-ffc8-bd4574af2b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=72ea4768c57df806f6a345b3e256f94722555c5902108d39f0c403c416037ce5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inicialização da Sessão Spark**\n",
        "## Criação de uma sessão do Spark, para processar os dados no framework."
      ],
      "metadata": {
        "id": "9cvtkx1xzZ4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "GjQJEOKJLKu7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Contador de Palavras\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "QNcGqX9fLWRj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Leitura do Arquivo de Texto**\n",
        "## O arquivo de texto é carregado no Spark para ser processado. Neste caso, o arquivo README.md foi utilizado."
      ],
      "metadata": {
        "id": "nvO6JV7PzrlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "f_ChZddgLgM9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "sh7hsxuZLpck",
        "outputId": "8678f388-82c5-4b4d-b0e9-65b57ad498c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dd770d65-deba-4ce1-94de-5b9fb887e48e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dd770d65-deba-4ce1-94de-5b9fb887e48e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving README.md to README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Processamento de Dados e Contagem de Palavras**\n",
        "## O arquivo é processado com Spark RDDs para dividir o texto em palavras e calcular o número de ocorrências de cada uma."
      ],
      "metadata": {
        "id": "rZ7qd7P3z9m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = spark.read.text(\"README.md\")\n",
        "\n",
        "words = text_file.rdd.flatMap(lambda line: line[0].split())\n",
        "\n",
        "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "results = word_counts.collect()\n",
        "for word, count in results:\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afwyMGkzxnVI",
        "outputId": "e5764e5e-e4f9-4a3d-ea05-e0a7b3bd8e07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##: 1\n",
            "Na: 1\n",
            "Sprint: 1\n",
            "7: 1\n",
            "foi: 3\n",
            "desenvolvido:: 1\n",
            "A: 1\n",
            "parte: 1\n",
            "2: 2\n",
            "da: 2\n",
            "entrega: 2\n",
            "do: 8\n",
            "desafio,: 1\n",
            "ingestão: 1\n",
            "po: 1\n",
            "api,: 1\n",
            "além: 1\n",
            "de: 20\n",
            "exercicios: 1\n",
            "no: 9\n",
            "laboratório: 1\n",
            "Glue,: 1\n",
            "leitura: 1\n",
            "sobre: 2\n",
            "o: 5\n",
            "hadoop,: 1\n",
            "também: 2\n",
            "pude: 1\n",
            "botar: 1\n",
            "em: 5\n",
            "prática: 1\n",
            "que: 1\n",
            "aprendido: 1\n",
            "curso: 4\n",
            "spark: 1\n",
            "exercicio: 1\n",
            "contador: 1\n",
            "palavras: 1\n",
            "com: 4\n",
            "pyspark.: 1\n",
            "1.: 1\n",
            "**No: 1\n",
            "Spark: 6\n",
            "foram: 2\n",
            "abordados: 1\n",
            "os: 4\n",
            "seguintes: 1\n",
            "tópicos:**: 1\n",
            "-: 22\n",
            "DataFrames: 1\n",
            "e: 10\n",
            "RDDS: 1\n",
            "SQL: 1\n",
            "Aplicações: 1\n",
            "Otimizações: 1\n",
            "Outros: 1\n",
            "aspectos: 1\n",
            "como:: 1\n",
            "notebook: 1\n",
            "Jupyter: 1\n",
            "Converter: 1\n",
            "Pandas: 1\n",
            "pra: 1\n",
            "DataFrame: 1\n",
            "UI: 1\n",
            "2.: 1\n",
            "**Na: 1\n",
            "Desafio: 1\n",
            "desenvolvido:**: 1\n",
            "Configuração: 2\n",
            "API: 3\n",
            "acesso: 1\n",
            "à: 2\n",
            "TMDB,: 1\n",
            "incluindo: 2\n",
            "armazenamento: 1\n",
            "seguro: 1\n",
            "credenciais: 1\n",
            "um: 4\n",
            "arquivo: 1\n",
            ".env.: 1\n",
            "Desenvolvimento: 1\n",
            "Script: 1\n",
            "Ingestão: 1\n",
            "Criação: 1\n",
            "script: 1\n",
            "Python: 1\n",
            "(API-ingestao.py): 1\n",
            "responsável: 1\n",
            "por: 2\n",
            "realizar: 1\n",
            "chamadas: 1\n",
            "TMDB: 1\n",
            "coletar: 1\n",
            "dados: 4\n",
            "filmes: 1\n",
            "séries,: 1\n",
            "agrupando: 1\n",
            "as: 2\n",
            "informações: 2\n",
            "arquivos: 1\n",
            "JSON: 1\n",
            "limite: 1\n",
            "100: 1\n",
            "registros: 1\n",
            "arquivo.: 1\n",
            "Estruturação: 1\n",
            "Dados: 1\n",
            "Os: 1\n",
            "organizados: 1\n",
            "diretórios: 1\n",
            "específicos: 1\n",
            "Amazon: 1\n",
            "S3,: 1\n",
            "seguindo: 1\n",
            "a: 7\n",
            "estrutura: 1\n",
            "pastas: 1\n",
            "definida: 1\n",
            "para: 3\n",
            "facilitar: 1\n",
            "consulta: 1\n",
            "análise: 1\n",
            "futura.: 1\n",
            "Consultas: 1\n",
            "Análises: 1\n",
            "Realização: 1\n",
            "várias: 1\n",
            "consultas: 1\n",
            "AWS: 5\n",
            "Athena: 1\n",
            "validar: 1\n",
            "analisar: 1\n",
            "inseridos,: 1\n",
            "contagens: 1\n",
            "médias,: 1\n",
            "visando: 1\n",
            "identificar: 1\n",
            "qualidade: 1\n",
            "riqueza: 1\n",
            "das: 1\n",
            "[Desafio]: 1\n",
            "(./Desafio): 1\n",
            "3.: 1\n",
            "**Lab: 1\n",
            "Glue**: 1\n",
            "Construção: 1\n",
            "processo: 1\n",
            "ETL: 1\n",
            "simplificado: 1\n",
            "utilizando: 1\n",
            "serviço: 1\n",
            "Glue: 3\n",
            "Preparar: 1\n",
            "origem: 1\n",
            "Criar: 3\n",
            "IAM: 1\n",
            "Role: 1\n",
            "jobs: 1\n",
            "Configurar: 1\n",
            "permissões: 1\n",
            "Lake: 1\n",
            "Formation: 1\n",
            "novo: 2\n",
            "job: 1\n",
            "Crawler: 1\n",
            "4.: 1\n",
            "**Contador: 1\n",
            "Palavras: 1\n",
            "Apache: 1\n",
            "Spark**: 1\n",
            "[Exercicios]: 1\n",
            "(./Exercicios): 1\n",
            "Também: 1\n",
            "acabei: 1\n",
            "vendo: 1\n",
            "ML: 1\n",
            "outros: 1\n",
            "tópicos: 1\n",
            "completo: 1\n",
            "outro: 1\n",
            "preparatório: 1\n",
            "[Certificados]: 1\n",
            "(./Certificados): 1\n",
            "E: 1\n",
            "execução: 1\n",
            "todos: 1\n",
            "passos: 1\n",
            "estão: 1\n",
            "na: 1\n",
            "pasta: 1\n",
            "evidencias: 1\n",
            "[Evidencias]: 1\n",
            "(./Evidencias): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encerramento da Sessão Spark**\n",
        "## Após o processamento, a sessão Spark é encerrada para liberar os recursos."
      ],
      "metadata": {
        "id": "h3oER0TA0Ta6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "KkPv-Z510PwJ"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}